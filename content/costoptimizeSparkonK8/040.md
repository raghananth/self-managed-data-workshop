---
title: "Submit a Spark job via command line"
weight: 40
---
## Submit a Spark job via command line

We have included a second job manifest file and a Jupyter notebook, as an example of a complex Spark job to solve a real-world data problem. Let's submit it via a command line this time. 
<details>
<summary>manifest file</summary>
The [manifest file](/source/example/scd2-workflow-job.yaml) defines where the Jupyter notebook file (job configuration) and input data are. 
</details>
<details>
<summary>Jupyter notebook</summary>
The [Jupyter notebook](/source/example/scd2_job.ipynb) specifies what exactly need to do in a data pipeline.
</details>

In general, parquet files are immutable in Data Lake. This example will demonstrate how to address the problem and process data incrementally. It uses `Delta Lake`, an open source storage layer on top of parquet file, to bring the ACID transactions to your modern data architecture. In the example, we will build up a table to support the [Slowly Changing Dimension Type 2](https://www.datawarehouse4u.info/SCD-Slowly-Changing-Dimensions.html) format, to demostrate how easy to do the ETL with a SQL first approach implemented in a configuration-driven way.


1. Open the [scd2 job manifest file](source/example/scd2-workflow-job.yaml) on your computer, replace the S3 bucket placeholder {{codeBucket}}. Either copy & paste the code bucket name from your deployment output, or use the existing s3 bucket name passed in as a parameter to your deployment previously.

```
vi source/example/scd2-workflow-job.yaml
```
![](/images/4-cfn-output.png)
![](/images/3-scd-bucket.png)


2. Submit Arc Spark job

```
# Make sure you are in the project root directory
cd sql-based-etl-with-apache-spark-on-amazon-eks

# Start a data pipeline containing 3 spark jobs
kubectl apply -f source/example/scd2-workflow-job.yaml

# Delete before submit the same job again
kubectl delete -f source/example/scd2-workflow-job.yaml
kubectl apply -f source/example/scd2-workflow-job.yaml
```

<details>
<summary> 
Alternatively, submit the job with Argo CLI. 
</summary> 

```
argo submit source/example/scd2-workflow-job.yaml -n spark --watch

# terminate your job, for example 'scd2-job-vvkgr'
argo delete scd2-job-<random_string> -n spark

```
*** Make sure to comment out a line in the manifest file before your submission. ***
![](/images/2-comment-out.png)
![](/images/2-argo-scdjob.png)
</details>


3. Go to your Argo dashboard via the deployment output URL link

![](/images/0-argo-uri.png)



4. Check the job status and applications logs
![](/images/3-argo-log.png)



## Develop & test Spark job in Jupyter

Apart from orchestrating Spark jobs with a declarative approach, we introduce a configuration-driven design for increasing data process productivity, by leveraging an open-source [data framework Arc](https://arc.tripl.ai/) for a SQL-centric ETL solution. We take considerations of the needs and expected skills from our customers in data, and accelerate their interaction with ETL practice in order to foster simplicity, while maximizing efficiency.

1. Login to Jupyter Hub user interface:

![](/images/3-jupyter-url.png)

username -  `sparkoneks`, or use your own login name defined at the deployment earlier. 
password - see below command.

```
JHUB_PWD=$(kubectl -n jupyter get secret jupyter-external-secret -o jsonpath="{.data.password}" | base64 --decode)
echo -e "\njupyter login: $JHUB_PWD"
```

2. Start the default IDE (development environment). Use the bigger instance to author your ETL job if needed.

![](/images/3-jhub-login.png)


3. Upload a sample Arc-jupyter notebook from `source/example/scd2_job.ipynb`

![](/images/3-jhub-open-notebook.png)


4. Execute each block and observe the result.

NOTE: the variable `${ETL_CONF_DATALAKE_LOC}` is set to a code bucket or an existing DataLake S3 bucket specified at the deployment. An IAM role attached to the Jupyter Hub is controling the data access to the S3 bucket. If you would like to connect to another bucket that wansn't specified at the deployment, you will get an access deny error. In this case, simply add the bucket ARN to the IAM role 'SparkOnEKS-EksClusterjhubServiceAcctRole'.
