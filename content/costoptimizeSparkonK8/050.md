---
title: "Submit a native Spark job with Spot instance"
weight: 50
---

## Submit a native Spark job with Spot instance

As an addition of the solution, to meet customer's preference of running native Spark jobs, we will demonstrate how easy to submit a job with a fat jar file in EKS. In this case, the Jupyter notebook still can be used, as an interactive development environment for PySpark apps. 

In Spark, driver is a single point of failure in data processing. If driver dies, all other linked components will be discarded as well. To achieve the optimal performance and cost, we will run the driver on a reliable managed EC2 instance on EKS, and the rest of executors will be on spot instances.

1. [OPTIONAL] Run a dummy container to validate the spot template file. 
NOTE: update the placeholder with corret information.

```
kubectl run --generator=run-pod/v1 jump-pod --rm -i --tty --serviceaccount=nativejob --namespace=spark --image <your_account_number>.dkr.ecr.<your_region>.amazonaws.com/arc:latest sh

# after login to the container, validate a file
# ensure the service account is `nativejob` and the lifecycle value is `Ec2Spot` in the template
sh-5.0# ls 
sh-5.0# cat executor-pod-template.yaml
sh-5.0# exit

```

2. Modify the job manifest file [native-spark-job.yaml](source/example/native-spark-job.yaml) stored on your computer, ie. replace the placeholder {{codeBucket}}.

![](/images/4-cfn-output.png)
![](/images/4-spark-output-s3.png)


3. Submit the native Spark job

```
kubectl apply -f source/example/native-spark-job.yaml

```
4. Go to SparkUI to check your job progress and performance

```
driver=$(kubectl get pod -n spark -l spark-role=driver -o jsonpath="{.items[*].metadata.name}")
kubectl port-forward $driver 4040:4040 -n spark

# go to `localhost:4040` from your web browser

```
5. Examine the auto-scaling and multi-AZs

```
# The job requests 5 Spark executors (pods) on top of 5 spot instances. It is configurable to fit in to a single or less number of spot instances. 
# the auto-scaling is triggered across multiple AZs, again it is configurable to trigger the job in a single AZ if reuqired.

kubectl get node --label-columns=lifecycle,topology.kubernetes.io/zone
kubectl get pod -n spark
```
![](/images/4-auto-scaling.png)