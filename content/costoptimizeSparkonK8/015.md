---
title: "Deploy Infrastructure"
weight: 15
---

## Deploy Infrastructure

Clone the project.

```
git clone https://github.com/awslabs/sql-based-etl-with-apache-spark-on-amazon-eks.git
cd sql-based-etl-with-apache-spark-on-amazon-eks

```

Build Arc docker image and push it to your container registry ECR. 
The following bash script will help you to prepare the deployment in your AWS environment. Replace the following placeholders by your own information.

```
bash deployment/pull_and_push_ecr.sh <your_region> <your_account_number> 'arc'

# use `skip_ecr` flag, when rerun the script without docker push 
bash deployment/pull_and_push_ecr.sh <your_region> <your_account_number> 'arc' 'skip_ecr'
```

Install kubernetes tools on MAC.
If you are running Linux / Windows, please see the [official docs](https://github.com/argoproj/argo/releases) for the download links.

```
bash deployment/setup_cmd_tool.sh
```

Manually create a virtualenv on MacOS and Linux.
This project is set up like a standard Python project. The `cdk.json` file tells where the application entry point is.

``` 
python3 -m venv .env
```
If you are in a Windows platform, you would activate the virtualenv like this:

```
% .env\Scripts\activate.bat
```
After the virtualenv is created, you can use the followings to activate your virtualenv and install the required dependencies.

```
source .env/bin/activate
pip install -r source/requirements.txt
```
Finally deploy the stack. It takes two optional parameters `jhubuser` & `datalakebucket`.

```
# Scenario1: Deploy with default settings (recommended)

cdk deploy SparkOnEKS -c env=develop

# Scenario2: Create a username to login Jupyter hub. 
# Otherwise, login by a default name.

cdk deploy SparkOnEKS -c env=develop --parameters jhubuser=<random_login_name>

# Scenario3: by default, the `datalakebucket` is pointing to a S3 bucket created by the solution.
# if you prefer to use an existing bucket that contains real data, replace the placeholder by your bucket name. 
# NOTE: the bucket must be in the same region as the solution deployment region.

cdk deploy SparkOnEKS -c env=develop --parameters jhubuser=<random_login_name> --parameters datalakebucket=<existing_datalake_bucket>

```
## Troubleshooting

1. If you see the issue `[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1123)`, most likely it means no default certificate authority for your Python installation on OSX. Refer to the [answer](https://stackoverflow.com/questions/52805115/0nd installing `Install Certificates.command` should fix your local environment. Otherwise, use [Cloud9](https://aws.amazon.com/cloud9/details/) to deploy the CDK instead.

2. If an error says `SparkOnEKS failed: Error: This stack uses assets, so the toolkit stack must be deployed to the environment (Run "cdk bootstrap aws://YOUR_ACCOUNT_NUMBER/REGION")` , it means it is the first time you deploy an AWS CDK app into an environment (account/region), you’ll need to install a “bootstrap stack”. This stack includes resources that are needed for the toolkit’s operation. For example, the stack includes an S3 bucket that is used to store templates and assets during the deployment process.

You can use the cdk bootstrap command to install the bootstrap stack into an environment:

```
cdk bootstrap aws://<YOUR_ACCOUNT_NUMBER>/<YOUR_REGION> -c develop
```
